{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Students_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "NnN0OokBT1M1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z-M0rahtUCFq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"Multi-Layer Perceptron Class\n",
    "\n",
    "    A multilayer perceptron is a feedforward artificial neural network model\n",
    "    that has one layer or more of hidden units and nonlinear activations.\n",
    "    Intermediate layers usually have as activation function tanh or the\n",
    "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
    "    class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
    "        # into a HiddenLayer with a tanh activation function connected to the\n",
    "        # LogisticRegression layer; the activation function can be replaced by\n",
    "        # sigmoid or any other nonlinear function\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "\n",
    "        # The logistic regression layer gets as input the hidden units\n",
    "        # of the hidden layer\n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        # end-snippet-2 start-snippet-3\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1 = (\n",
    "                abs(self.hiddenLayer.W).sum()\n",
    "                + abs(self.logRegressionLayer.W).sum()\n",
    "        )\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr = (\n",
    "                (self.hiddenLayer.W ** 2).sum()\n",
    "                + (self.logRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "\n",
    "        # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "\n",
    "        # the parameters of the model are the parameters of the two layer it is\n",
    "        # made out of\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        # end-snippet-3\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sKEPy9sXUKhH",
    "colab_type": "code",
    "outputId": "b1f9a99b-598e-4fbc-ea32-9bc21493de50",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# from mlpstudent import MLP\n",
    "\n",
    "\n",
    "# Read the data\n",
    "students_url = 'https://raw.githubusercontent.com/tomazellifelipe/PIBITI/master/student-por.csv'\n",
    "students_data = pd.read_csv(students_url, sep=';', true_values=['yes'], false_values=['no'])\n",
    "\n",
    "# Add boolean column PassFail\n",
    "def passfail(row):\n",
    "    if row['G3'] >= 10:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "students_data['PassFail'] = students_data.apply(lambda row: passfail(row), axis=1)\n",
    "\n",
    "# data visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "students_data.G3.hist(ax=axes[0])\n",
    "axes[0].set_title(\"Distribuição de G3\")\n",
    "sns.countplot(x=\"PassFail\", data=students_data, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuição entre Aprovados/Reprovados\")\n",
    "plt.grid(True, axis='y')\n",
    "# plt.show()\n",
    "\n",
    "# Separate target from predictors\n",
    "Y = students_data.PassFail\n",
    "X = students_data.drop(['absences', 'G1', 'G2', 'G3', 'PassFail'], axis=1)\n",
    "\n",
    "# columns types\n",
    "students_num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "students_cat_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and\n",
    "                     X[cname].dtype == \"object\"]\n",
    "students_bool_cols = [cname for cname in X.columns if X[cname].dtype == \"bool\"]\n",
    "\n",
    "\n",
    "def scale_numeric(data, numeric_columns, scaler):\n",
    "    for col in numeric_columns:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "# We can now define the scaler we want to use and apply it to our dataset\n",
    "scaler = StandardScaler()\n",
    "X = scale_numeric(X, [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']], scaler)\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = students_cat_cols + students_num_cols + students_bool_cols\n",
    "X = X[my_cols].copy()\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, students_num_cols),\n",
    "        ('cat', categorical_transformer, students_cat_cols),\n",
    "        ('bool', 'passthrough', students_bool_cols)])\n",
    "\n",
    "X = pd.DataFrame(preprocessor.fit_transform(X))\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "# Divide data into training, validation and test subsets\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_valid, Y_valid, train_size=0.8, test_size=0.2,\n",
    "                                                    shuffle=True)\n",
    "train_set = (X_train, Y_train)\n",
    "valid_set = (X_valid, Y_valid)\n",
    "test_set = (X_test, Y_test)\n",
    "\n",
    "\n",
    "def shared_dataset(data_xy, borrow=True):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=borrow)\n",
    "    shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=borrow).flatten()\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ``floatX`` as well\n",
    "    # (``shared_y`` does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn't make sense) therefore instead of returning\n",
    "    # ``shared_y`` we will have to cast it to int. This little hack\n",
    "    # lets ous get around this issue\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "\n",
    "te_set_x, te_set_y = shared_dataset(test_set)\n",
    "val_set_x, val_set_y = shared_dataset(valid_set)\n",
    "tr_set_x, tr_set_y = shared_dataset(train_set)\n",
    "dataset = [(tr_set_x, tr_set_y), (val_set_x, val_set_y), (te_set_x, te_set_y)]\n",
    "\n",
    "# TUDO CERTO ATÉ AQUI ==================================================================================================\n",
    "\n",
    "\n",
    "def test_mlp(datasets, learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000, batch_size=1, n_hidden=500):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "    perceptron\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :param datasets: previous dataset\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "    gradient\n",
    "\n",
    "    :type L1_reg: float\n",
    "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type L2_reg: float\n",
    "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "    # [int] labels\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = MLP(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_in=47,\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=10\n",
    "    )\n",
    "\n",
    "    # start-snippet-4\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "    # here symbolically\n",
    "    cost = (\n",
    "            classifier.negative_log_likelihood(y)\n",
    "            + L1_reg * classifier.L1\n",
    "            + L2_reg * classifier.L2_sqr\n",
    "    )\n",
    "    # end-snippet-4\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made\n",
    "    # by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # start-snippet-5\n",
    "    # compute the gradient of cost with respect to theta (sorted in params)\n",
    "    # the resulting gradients will be stored in a list gparams\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs\n",
    "\n",
    "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
    "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
    "    # element is a pair formed from the two lists :\n",
    "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-5\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "    # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "    # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "    # go through this many\n",
    "    # minibatche before checking the network\n",
    "    # on the validation set; in this case we\n",
    "    # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                            this_validation_loss < best_validation_loss *\n",
    "                            improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                   in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "           'obtained at iteration %i, with test performance %f %%') %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print(('The code for file ' +\n",
    "           os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n",
    "\n",
    "\n",
    "test_mlp(dataset)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "... building the model\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "... training\n",
      "epoch 1, minibatch 519/519, validation error 16.346154 %\n",
      "     epoch 1, minibatch 519/519, test error of best model 30.769231 %\n",
      "epoch 2, minibatch 519/519, validation error 15.384615 %\n",
      "     epoch 2, minibatch 519/519, test error of best model 30.769231 %\n",
      "epoch 3, minibatch 519/519, validation error 14.423077 %\n",
      "     epoch 3, minibatch 519/519, test error of best model 30.769231 %\n",
      "epoch 4, minibatch 519/519, validation error 13.461538 %\n",
      "     epoch 4, minibatch 519/519, test error of best model 30.769231 %\n",
      "epoch 5, minibatch 519/519, validation error 12.500000 %\n",
      "     epoch 5, minibatch 519/519, test error of best model 30.769231 %\n",
      "epoch 6, minibatch 519/519, validation error 14.423077 %\n",
      "epoch 7, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 8, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 9, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 10, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 11, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 12, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 13, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 14, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 15, minibatch 519/519, validation error 15.384615 %\n",
      "epoch 16, minibatch 519/519, validation error 14.423077 %\n",
      "epoch 17, minibatch 519/519, validation error 14.423077 %\n",
      "epoch 18, minibatch 519/519, validation error 14.423077 %\n",
      "epoch 19, minibatch 519/519, validation error 13.461538 %\n",
      "Optimization complete. Best validation score of 12.500000 % obtained at iteration 2595, with test performance 30.769231 %\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3f3e5417a6c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3f3e5417a6c8>\u001b[0m in \u001b[0;36mtest_mlp\u001b[0;34m(datasets, learning_rate, L1_reg, L2_reg, n_epochs, batch_size, n_hidden)\u001b[0m\n\u001b[1;32m    328\u001b[0m           (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n\u001b[1;32m    329\u001b[0m     print(('The code for file ' +\n\u001b[0;32m--> 330\u001b[0;31m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m            ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAGDCAYAAAAoFdb3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkyMJUmSJMDEWJIkSQLg/wNj5u++TxHcBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  }
 ]
}