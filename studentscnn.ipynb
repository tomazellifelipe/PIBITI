{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "studentscnn",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDpYBe1AX8Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "\n",
        "class LogisticRegression(object):\n",
        "    \"\"\"Multi-class Logistic Regression Class\n",
        "\n",
        "    The logistic regression is fully described by a weight matrix :math:`W`\n",
        "    and bias vector :math:`b`. Classification is done by projecting data\n",
        "    points onto a set of hyperplanes, the distance to which is used to\n",
        "    determine a class membership probability.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input, n_in, n_out):\n",
        "        \"\"\" Initialize the parameters of the logistic regression\n",
        "\n",
        "        :type input: theano.tensor.TensorType\n",
        "        :param input: symbolic variable that describes the input of the\n",
        "                      architecture (one minibatch)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: number of input units, the dimension of the space in\n",
        "                     which the datapoints lie\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of output units, the dimension of the space in\n",
        "                      which the labels lie\n",
        "\n",
        "        \"\"\"\n",
        "        # start-snippet-1\n",
        "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
        "        self.W = theano.shared(\n",
        "            value=numpy.zeros(\n",
        "                (n_in, n_out),\n",
        "                dtype=theano.config.floatX\n",
        "            ),\n",
        "            name='W',\n",
        "            borrow=True\n",
        "        )\n",
        "        # initialize the biases b as a vector of n_out 0s\n",
        "        self.b = theano.shared(\n",
        "            value=numpy.zeros(\n",
        "                (n_out,),\n",
        "                dtype=theano.config.floatX\n",
        "            ),\n",
        "            name='b',\n",
        "            borrow=True\n",
        "        )\n",
        "\n",
        "        # symbolic expression for computing the matrix of class-membership\n",
        "        # probabilities\n",
        "        # Where:\n",
        "        # W is a matrix where column-k represent the separation hyperplane for\n",
        "        # class-k\n",
        "        # x is a matrix where row-j  represents input training sample-j\n",
        "        # b is a vector where element-k represent the free parameter of\n",
        "        # hyperplane-k\n",
        "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
        "\n",
        "        # symbolic description of how to compute prediction as class whose\n",
        "        # probability is maximal\n",
        "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
        "        # end-snippet-1\n",
        "\n",
        "        # parameters of the model\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "        # keep track of model input\n",
        "        self.input = input\n",
        "\n",
        "    def negative_log_likelihood(self, y):\n",
        "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
        "        of this model under a given target distribution.\n",
        "\n",
        "        .. math::\n",
        "\n",
        "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
        "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
        "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
        "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
        "\n",
        "        :type y: theano.tensor.TensorType\n",
        "        :param y: corresponds to a vector that gives for each example the\n",
        "                  correct label\n",
        "\n",
        "        Note: we use the mean instead of the sum so that\n",
        "              the learning rate is less dependent on the batch size\n",
        "        \"\"\"\n",
        "        # start-snippet-2\n",
        "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
        "        # number of examples (call it n) in the minibatch\n",
        "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
        "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
        "        # Log-Probabilities (call it LP) with one row per example and\n",
        "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
        "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
        "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
        "        # the mean (across minibatch examples) of the elements in v,\n",
        "        # i.e., the mean log-likelihood across the minibatch.\n",
        "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
        "        # end-snippet-2\n",
        "\n",
        "    def errors(self, y):\n",
        "        \"\"\"Return a float representing the number of errors in the minibatch\n",
        "        over the total number of examples of the minibatch ; zero one\n",
        "        loss over the size of the minibatch\n",
        "\n",
        "        :type y: theano.tensor.TensorType\n",
        "        :param y: corresponds to a vector that gives for each example the\n",
        "                  correct label\n",
        "        \"\"\"\n",
        "\n",
        "        # check if y has same dimension of y_pred\n",
        "        if y.ndim != self.y_pred.ndim:\n",
        "            raise TypeError(\n",
        "                'y should have the same shape as self.y_pred',\n",
        "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
        "            )\n",
        "        # check if y is of the correct datatype\n",
        "        if y.dtype.startswith('int'):\n",
        "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
        "            # represents a mistake in prediction\n",
        "            return T.mean(T.neq(self.y_pred, y))\n",
        "        else:\n",
        "            raise NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLMkP1JYJ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import timeit\n",
        "import numpy\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "# from logisticstudent import LogisticRegression\n",
        "\n",
        "\n",
        "class HiddenLayer(object):\n",
        "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
        "                 activation=T.tanh):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type rng: numpy.random.RandomState\n",
        "        :param rng: a random number generator used to initialize weights\n",
        "\n",
        "        :type input: theano.tensor.dmatrix\n",
        "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: theano.Op or function\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        # end-snippet-1\n",
        "\n",
        "        # `W` is initialized with `W_values` which is uniformely sampled\n",
        "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
        "        # for tanh activation function\n",
        "        # the output of uniform if converted using asarray to dtype\n",
        "        # theano.config.floatX so that the code is runable on GPU\n",
        "        # Note : optimal initialization of weights is dependent on the\n",
        "        #        activation function used (among other things).\n",
        "        #        For example, results presented in [Xavier10] suggest that you\n",
        "        #        should use 4 times larger initial weights for sigmoid\n",
        "        #        compared to tanh\n",
        "        #        We have no info for other function, so we use the same as\n",
        "        #        tanh.\n",
        "        if W is None:\n",
        "            W_values = numpy.asarray(\n",
        "                rng.uniform(\n",
        "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
        "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
        "                    size=(n_in, n_out)\n",
        "                ),\n",
        "                dtype=theano.config.floatX\n",
        "            )\n",
        "            if activation == theano.tensor.nnet.sigmoid:\n",
        "                W_values *= 4\n",
        "\n",
        "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
        "\n",
        "        if b is None:\n",
        "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
        "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
        "\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        lin_output = T.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if activation is None\n",
        "            else activation(lin_output)\n",
        "        )\n",
        "        # parameters of the model\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "\n",
        "class MLP(object):\n",
        "    \"\"\"Multi-Layer Perceptron Class\n",
        "\n",
        "    A multilayer perceptron is a feedforward artificial neural network model\n",
        "    that has one layer or more of hidden units and nonlinear activations.\n",
        "    Intermediate layers usually have as activation function tanh or the\n",
        "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
        "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
        "    class).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
        "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
        "\n",
        "        :type rng: numpy.random.RandomState\n",
        "        :param rng: a random number generator used to initialize weights\n",
        "\n",
        "        :type input: theano.tensor.TensorType\n",
        "        :param input: symbolic variable that describes the input of the\n",
        "        architecture (one minibatch)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: number of input units, the dimension of the space in\n",
        "        which the datapoints lie\n",
        "\n",
        "        :type n_hidden: int\n",
        "        :param n_hidden: number of hidden units\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of output units, the dimension of the space in\n",
        "        which the labels lie\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
        "        # into a HiddenLayer with a tanh activation function connected to the\n",
        "        # LogisticRegression layer; the activation function can be replaced by\n",
        "        # sigmoid or any other nonlinear function\n",
        "        self.hiddenLayer = HiddenLayer(\n",
        "            rng=rng,\n",
        "            input=input,\n",
        "            n_in=n_in,\n",
        "            n_out=n_hidden,\n",
        "            activation=T.tanh\n",
        "        )\n",
        "\n",
        "        # The logistic regression layer gets as input the hidden units\n",
        "        # of the hidden layer\n",
        "        self.logRegressionLayer = LogisticRegression(\n",
        "            input=self.hiddenLayer.output,\n",
        "            n_in=n_hidden,\n",
        "            n_out=n_out\n",
        "        )\n",
        "        # end-snippet-2 start-snippet-3\n",
        "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
        "        # be small\n",
        "        self.L1 = (\n",
        "                abs(self.hiddenLayer.W).sum()\n",
        "                + abs(self.logRegressionLayer.W).sum()\n",
        "        )\n",
        "\n",
        "        # square of L2 norm ; one regularization option is to enforce\n",
        "        # square of L2 norm to be small\n",
        "        self.L2_sqr = (\n",
        "                (self.hiddenLayer.W ** 2).sum()\n",
        "                + (self.logRegressionLayer.W ** 2).sum()\n",
        "        )\n",
        "\n",
        "        # negative log likelihood of the MLP is given by the negative\n",
        "        # log likelihood of the output of the model, computed in the\n",
        "        # logistic regression layer\n",
        "        self.negative_log_likelihood = (\n",
        "            self.logRegressionLayer.negative_log_likelihood\n",
        "        )\n",
        "        # same holds for the function computing the number of errors\n",
        "        self.errors = self.logRegressionLayer.errors\n",
        "\n",
        "        # the parameters of the model are the parameters of the two layer it is\n",
        "        # made out of\n",
        "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
        "        # end-snippet-3\n",
        "\n",
        "        # keep track of model input\n",
        "        self.input = input\n",
        "\n",
        "\n",
        "def test_mlp(datasets, learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000, batch_size=1, n_hidden=500):\n",
        "    \"\"\"\n",
        "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
        "    perceptron\n",
        "\n",
        "    This is demonstrated on MNIST.\n",
        "\n",
        "    :param batch_size:\n",
        "    :param n_hidden:\n",
        "    :param datasets: previous dataset\n",
        "    :type learning_rate: float\n",
        "    :param learning_rate: learning rate used (factor for the stochastic\n",
        "    gradient\n",
        "\n",
        "    :type L1_reg: float\n",
        "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
        "    regularization)\n",
        "\n",
        "    :type L2_reg: float\n",
        "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
        "    regularization)\n",
        "\n",
        "    :type n_epochs: int\n",
        "    :param n_epochs: maximal number of epochs to run the optimizer\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "    train_set_x, train_set_y = datasets[0]\n",
        "    valid_set_x, valid_set_y = datasets[1]\n",
        "    test_set_x, test_set_y = datasets[2]\n",
        "\n",
        "    # compute number of minibatches for training, validation and testing\n",
        "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "\n",
        "    ######################\n",
        "    # BUILD ACTUAL MODEL #\n",
        "    ######################\n",
        "    print('... building the model')\n",
        "\n",
        "    # allocate symbolic variables for the data\n",
        "    index = T.lscalar()  # index to a [mini]batch\n",
        "    x = T.matrix('x')  # the data is presented as rasterized images\n",
        "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
        "    # [int] labels\n",
        "\n",
        "    rng = numpy.random.RandomState(1234)\n",
        "\n",
        "    # construct the MLP class\n",
        "    classifier = MLP(\n",
        "        rng=rng,\n",
        "        input=x,\n",
        "        n_in=47,\n",
        "        n_hidden=n_hidden,\n",
        "        n_out=10\n",
        "    )\n",
        "\n",
        "    # start-snippet-4\n",
        "    # the cost we minimize during training is the negative log likelihood of\n",
        "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
        "    # here symbolically\n",
        "    cost = (\n",
        "            classifier.negative_log_likelihood(y)\n",
        "            + L1_reg * classifier.L1\n",
        "            + L2_reg * classifier.L2_sqr\n",
        "    )\n",
        "    # end-snippet-4\n",
        "\n",
        "    # compiling a Theano function that computes the mistakes that are made\n",
        "    # by the model on a minibatch\n",
        "    test_model = theano.function(\n",
        "        inputs=[index],\n",
        "        outputs=classifier.errors(y),\n",
        "        givens={\n",
        "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
        "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    validate_model = theano.function(\n",
        "        inputs=[index],\n",
        "        outputs=classifier.errors(y),\n",
        "        givens={\n",
        "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
        "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # start-snippet-5\n",
        "    # compute the gradient of cost with respect to theta (sorted in params)\n",
        "    # the resulting gradients will be stored in a list gparams\n",
        "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
        "\n",
        "    # specify how to update the parameters of the model as a list of\n",
        "    # (variable, update expression) pairs\n",
        "\n",
        "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
        "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
        "    # element is a pair formed from the two lists :\n",
        "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
        "    updates = [\n",
        "        (param, param - learning_rate * gparam)\n",
        "        for param, gparam in zip(classifier.params, gparams)\n",
        "    ]\n",
        "\n",
        "    # compiling a Theano function `train_model` that returns the cost, but\n",
        "    # in the same time updates the parameter of the model based on the rules\n",
        "    # defined in `updates`\n",
        "    train_model = theano.function(\n",
        "        inputs=[index],\n",
        "        outputs=cost,\n",
        "        updates=updates,\n",
        "        givens={\n",
        "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
        "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "    # end-snippet-5\n",
        "\n",
        "    ###############\n",
        "    # TRAIN MODEL #\n",
        "    ###############\n",
        "    print('... training')\n",
        "\n",
        "    # early-stopping parameters\n",
        "    patience = 10000  # look as this many examples regardless\n",
        "    patience_increase = 2  # wait this much longer when a new best is\n",
        "    # found\n",
        "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
        "    # considered significant\n",
        "    validation_frequency = min(n_train_batches, patience // 2)\n",
        "    # go through this many\n",
        "    # minibatche before checking the network\n",
        "    # on the validation set; in this case we\n",
        "    # check every epoch\n",
        "\n",
        "    best_validation_loss = numpy.inf\n",
        "    best_iter = 0\n",
        "    test_score = 0.\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    epoch = 0\n",
        "    done_looping = False\n",
        "\n",
        "    while (epoch < n_epochs) and (not done_looping):\n",
        "        epoch = epoch + 1\n",
        "        for minibatch_index in range(n_train_batches):\n",
        "\n",
        "            minibatch_avg_cost = train_model(minibatch_index)\n",
        "            # iteration number\n",
        "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
        "\n",
        "            if (iter + 1) % validation_frequency == 0:\n",
        "                # compute zero-one loss on validation set\n",
        "                validation_losses = [validate_model(i) for i\n",
        "                                     in range(n_valid_batches)]\n",
        "                this_validation_loss = numpy.mean(validation_losses)\n",
        "\n",
        "                print(\n",
        "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
        "                    (\n",
        "                        epoch,\n",
        "                        minibatch_index + 1,\n",
        "                        n_train_batches,\n",
        "                        this_validation_loss * 100.\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # if we got the best validation score until now\n",
        "                if this_validation_loss < best_validation_loss:\n",
        "                    # improve patience if loss improvement is good enough\n",
        "                    if (\n",
        "                            this_validation_loss < best_validation_loss *\n",
        "                            improvement_threshold\n",
        "                    ):\n",
        "                        patience = max(patience, iter * patience_increase)\n",
        "\n",
        "                    best_validation_loss = this_validation_loss\n",
        "                    best_iter = iter\n",
        "\n",
        "                    # test it on the test set\n",
        "                    test_losses = [test_model(i) for i\n",
        "                                   in range(n_test_batches)]\n",
        "                    test_score = numpy.mean(test_losses)\n",
        "\n",
        "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
        "                           'best model %f %%') %\n",
        "                          (epoch, minibatch_index + 1, n_train_batches,\n",
        "                           test_score * 100.))\n",
        "\n",
        "            if patience <= iter:\n",
        "                done_looping = True\n",
        "                break\n",
        "\n",
        "    end_time = timeit.default_timer()\n",
        "    print(('Optimization complete. Best validation score of %f %% '\n",
        "           'obtained at iteration %i, with test performance %f %%') %\n",
        "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
        "    print(('The code for file ' +\n",
        "           os.path.split(__file__)[1] +\n",
        "           ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwSZiC4zYeJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import timeit\n",
        "import numpy\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "from theano.tensor.signal import pool\n",
        "from theano.tensor.nnet import conv2d\n",
        "# from logisticstudent import LogisticRegression\n",
        "# from mlpstudent import HiddenLayer\n",
        "\n",
        "\n",
        "class LeNetConvPoolLayer(object):\n",
        "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
        "\n",
        "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(1, 2)):\n",
        "        \"\"\"\n",
        "        Allocate a LeNetConvPoolLayer with shared variable internal parameters.\n",
        "\n",
        "        :type rng: numpy.random.RandomState\n",
        "        :param rng: a random number generator used to initialize weights\n",
        "\n",
        "        :type input: theano.tensor.dtensor4\n",
        "        :param input: symbolic image tensor, of shape image_shape\n",
        "\n",
        "        :type filter_shape: tuple or list of length 4\n",
        "        :param filter_shape: (number of filters, num input feature maps,\n",
        "                              filter height, filter width)\n",
        "\n",
        "        :type image_shape: tuple or list of length 4\n",
        "        :param image_shape: (batch size, num input feature maps,\n",
        "                             image height, image width)\n",
        "\n",
        "        :type poolsize: tuple or list of length 2\n",
        "        :param poolsize: the downsampling (pooling) factor (#rows, #cols)\n",
        "        \"\"\"\n",
        "\n",
        "        assert image_shape[1] == filter_shape[1]\n",
        "        self.input = input\n",
        "\n",
        "        # there are \"num input feature maps * filter height * filter width\"\n",
        "        # inputs to each hidden unit\n",
        "        fan_in = numpy.prod(filter_shape[1:])\n",
        "        # each unit in the lower layer receives a gradient from:\n",
        "        # \"num output feature maps * filter height * filter width\" /\n",
        "        #   pooling size\n",
        "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) //\n",
        "                   numpy.prod(poolsize))\n",
        "        # initialize weights with random weights\n",
        "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
        "        self.W = theano.shared(\n",
        "            numpy.asarray(\n",
        "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
        "                dtype=theano.config.floatX\n",
        "            ),\n",
        "            borrow=True\n",
        "        )\n",
        "\n",
        "        # the bias is a 1D tensor -- one bias per output feature map\n",
        "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
        "        self.b = theano.shared(value=b_values, borrow=True)\n",
        "\n",
        "        # convolve input feature maps with filters\n",
        "        conv_out = conv2d(\n",
        "            input=input,\n",
        "            filters=self.W,\n",
        "            filter_shape=filter_shape,\n",
        "            input_shape=image_shape\n",
        "        )\n",
        "\n",
        "        # pool each feature map individually, using maxpooling\n",
        "        pooled_out = pool.pool_2d(\n",
        "            input=conv_out,\n",
        "            ds=poolsize,\n",
        "            ignore_border=True\n",
        "        )\n",
        "\n",
        "        # add the bias term. Since the bias is a vector (1D array), we first\n",
        "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
        "        # thus be broadcasted across mini-batches and feature map\n",
        "        # width & height\n",
        "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "\n",
        "        # store parameters of this layer\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "        # keep track of model input\n",
        "        self.input = input\n",
        "\n",
        "\n",
        "def evaluate_lenet5(datasets, learning_rate=0.1, n_epochs=1000, nkerns=[20, 50], batch_size=1):\n",
        "    \"\"\" Demonstrates lenet on MNIST dataset\n",
        "\n",
        "    :param datasets:\n",
        "    :param batch_size:\n",
        "    :type learning_rate: float\n",
        "    :param learning_rate: learning rate used (factor for the stochastic\n",
        "                          gradient)\n",
        "\n",
        "    :type n_epochs: int\n",
        "    :param n_epochs: maximal number of epochs to run the optimizer\n",
        "\n",
        "    :type nkerns: list of ints\n",
        "    :param nkerns: number of kernels on each layer\n",
        "    \"\"\"\n",
        "\n",
        "    train_set_x, train_set_y = datasets[0]\n",
        "    valid_set_x, valid_set_y = datasets[1]\n",
        "    test_set_x, test_set_y = datasets[2]\n",
        "\n",
        "    # compute number of minibatches for training, validation and testing\n",
        "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "\n",
        "    # allocate symbolic variables for the data\n",
        "    index = T.lscalar()  # index to a [mini]batch\n",
        "    x = T.matrix('x')  # the data is presented as rasterized images\n",
        "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
        "    # [int] labels\n",
        "\n",
        "    rng = numpy.random.RandomState(1234)\n",
        "\n",
        "    ######################\n",
        "    # BUILD ACTUAL MODEL #\n",
        "    ######################\n",
        "    print('... building the model')\n",
        "\n",
        "    # Reshape matrix of rasterized images of shape (batch_size, 28 * 28)\n",
        "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
        "    # (28, 28) is the size of MNIST images.\n",
        "    layer0_input = x.reshape((batch_size, 1, 1, 47))\n",
        "\n",
        "    # Construct the first convolutional pooling layer:\n",
        "    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
        "    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
        "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
        "    layer0 = LeNetConvPoolLayer(\n",
        "        rng,\n",
        "        input=layer0_input,\n",
        "        image_shape=(batch_size, 1, 1, 47),\n",
        "        filter_shape=(nkerns[0], 1, 1, 6),\n",
        "        poolsize=(1, 2)\n",
        "    )\n",
        "\n",
        "    # Construct the second convolutional pooling layer\n",
        "    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
        "    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
        "    # 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
        "    layer1 = LeNetConvPoolLayer(\n",
        "        rng,\n",
        "        input=layer0.output,\n",
        "        image_shape=(batch_size, nkerns[0], 1, 21),\n",
        "        filter_shape=(nkerns[1], nkerns[0], 1, 6),\n",
        "        poolsize=(1, 2)\n",
        "    )\n",
        "\n",
        "    # the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
        "    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
        "    # This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
        "    # or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
        "    layer2_input = layer1.output.flatten(2)\n",
        "\n",
        "    # construct a fully-connected sigmoidal layer\n",
        "    layer2 = HiddenLayer(\n",
        "        rng,\n",
        "        input=layer2_input,\n",
        "        n_in=nkerns[1] * 8,\n",
        "        n_out=500,\n",
        "        activation=T.tanh\n",
        "    )\n",
        "\n",
        "    # classify the values of the fully-connected sigmoidal layer\n",
        "    layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
        "\n",
        "    # the cost we minimize during training is the NLL of the model\n",
        "    cost = layer3.negative_log_likelihood(y)\n",
        "\n",
        "    # create a function to compute the mistakes that are made by the model\n",
        "    test_model = theano.function(\n",
        "        [index],\n",
        "        layer3.errors(y),\n",
        "        givens={\n",
        "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
        "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    validate_model = theano.function(\n",
        "        [index],\n",
        "        layer3.errors(y),\n",
        "        givens={\n",
        "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
        "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # create a list of all model parameters to be fit by gradient descent\n",
        "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
        "\n",
        "    # create a list of gradients for all model parameters\n",
        "    grads = T.grad(cost, params)\n",
        "\n",
        "    # train_model is a function that updates the model parameters by\n",
        "    # SGD Since this model has many parameters, it would be tedious to\n",
        "    # manually create an update rule for each model parameter. We thus\n",
        "    # create the updates list by automatically looping over all\n",
        "    # (params[i], grads[i]) pairs.\n",
        "    updates = [\n",
        "        (param_i, param_i - learning_rate * grad_i)\n",
        "        for param_i, grad_i in zip(params, grads)\n",
        "    ]\n",
        "\n",
        "    train_model = theano.function(\n",
        "        [index],\n",
        "        cost,\n",
        "        updates=updates,\n",
        "        givens={\n",
        "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
        "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
        "        }\n",
        "    )\n",
        "    # end-snippet-1\n",
        "\n",
        "    ###############\n",
        "    # TRAIN MODEL #\n",
        "    ###############\n",
        "    print('... training')\n",
        "    # early-stopping parameters\n",
        "    patience = 10000  # look as this many examples regardless\n",
        "    patience_increase = 2  # wait this much longer when a new best is\n",
        "    # found\n",
        "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
        "    # considered significant\n",
        "    validation_frequency = min(n_train_batches, patience // 2)\n",
        "    # go through this many\n",
        "    # minibatche before checking the network\n",
        "    # on the validation set; in this case we\n",
        "    # check every epoch\n",
        "\n",
        "    best_validation_loss = numpy.inf\n",
        "    best_iter = 0\n",
        "    test_score = 0.\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    epoch = 0\n",
        "    done_looping = False\n",
        "\n",
        "    while (epoch < n_epochs) and (not done_looping):\n",
        "        epoch = epoch + 1\n",
        "        for minibatch_index in range(n_train_batches):\n",
        "\n",
        "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
        "\n",
        "            if iter % 100 == 0:\n",
        "                print('training @ iter = ', iter)\n",
        "            cost_ij = train_model(minibatch_index)\n",
        "\n",
        "            if (iter + 1) % validation_frequency == 0:\n",
        "\n",
        "                # compute zero-one loss on validation set\n",
        "                validation_losses = [validate_model(i) for i\n",
        "                                     in range(n_valid_batches)]\n",
        "                this_validation_loss = numpy.mean(validation_losses)\n",
        "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
        "                      (epoch, minibatch_index + 1, n_train_batches,\n",
        "                       this_validation_loss * 100.))\n",
        "\n",
        "                # if we got the best validation score until now\n",
        "                if this_validation_loss < best_validation_loss:\n",
        "\n",
        "                    # improve patience if loss improvement is good enough\n",
        "                    if this_validation_loss < best_validation_loss * \\\n",
        "                            improvement_threshold:\n",
        "                        patience = max(patience, iter * patience_increase)\n",
        "\n",
        "                    # save best validation score and iteration number\n",
        "                    best_validation_loss = this_validation_loss\n",
        "                    best_iter = iter\n",
        "\n",
        "                    # test it on the test set\n",
        "                    test_losses = [\n",
        "                        test_model(i)\n",
        "                        for i in range(n_test_batches)\n",
        "                    ]\n",
        "                    test_score = numpy.mean(test_losses)\n",
        "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
        "                           'best model %f %%') %\n",
        "                          (epoch, minibatch_index + 1, n_train_batches,\n",
        "                           test_score * 100.))\n",
        "\n",
        "            if patience <= iter:\n",
        "                done_looping = True\n",
        "                break\n",
        "\n",
        "    end_time = timeit.default_timer()\n",
        "    print('Optimization complete.')\n",
        "    print('Best validation score of %f %% obtained at iteration %i, '\n",
        "          'with test performance %f %%' %\n",
        "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
        "    print(('The code for file ' +\n",
        "           os.path.split(__file__)[1] +\n",
        "           ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOh-sTYYiCt",
        "colab_type": "code",
        "outputId": "ca6dbe94-59bc-40b1-f7de-15b8866abdfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "# from mlpstudent import test_mlp\n",
        "# from cnnstudent import evaluate_lenet5\n",
        "\n",
        "\n",
        "# ======================================================================================================================\n",
        "# Read the data\n",
        "# students_data = pd.read_csv('student-por.csv', sep=';', true_values=['yes'], false_values=['no'])\n",
        "students_url = 'https://raw.githubusercontent.com/tomazellifelipe/PIBITI/master/student-por.csv'\n",
        "students_data = pd.read_csv(students_url, sep=';', true_values=['yes'], false_values=['no'])\n",
        "\n",
        "\n",
        "# Add boolean column PassFail\n",
        "def passfail(row):\n",
        "    if row['G3'] >= 10:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "students_data['PassFail'] = students_data.apply(lambda row: passfail(row), axis=1)\n",
        "\n",
        "# data visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "students_data.G3.hist(ax=axes[0])\n",
        "axes[0].set_title(\"Distribuição de G3\")\n",
        "sns.countplot(x=\"PassFail\", data=students_data, ax=axes[1])\n",
        "axes[1].set_title(\"Distribuição entre Aprovados/Reprovados\")\n",
        "plt.grid(True, axis='y')\n",
        "# plt.show()\n",
        "\n",
        "# Separate target from predictors\n",
        "Y = students_data.PassFail\n",
        "X = students_data.drop(['absences', 'G1', 'G2', 'G3', 'PassFail'], axis=1)\n",
        "\n",
        "# columns types\n",
        "students_num_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
        "students_cat_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and\n",
        "                     X[cname].dtype == \"object\"]\n",
        "students_bool_cols = [cname for cname in X.columns if X[cname].dtype == \"bool\"]\n",
        "\n",
        "\n",
        "def scale_numeric(data, numeric_columns, scaler):\n",
        "    for col in numeric_columns:\n",
        "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
        "    return data\n",
        "\n",
        "\n",
        "# We can now define the scaler we want to use and apply it to our dataset\n",
        "scaler = StandardScaler()\n",
        "X = scale_numeric(X, [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']], scaler)\n",
        "\n",
        "# Keep selected columns only\n",
        "my_cols = students_cat_cols + students_num_cols + students_bool_cols\n",
        "X = X[my_cols].copy()\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, students_num_cols),\n",
        "        ('cat', categorical_transformer, students_cat_cols),\n",
        "        ('bool', 'passthrough', students_bool_cols)])\n",
        "\n",
        "X = pd.DataFrame(preprocessor.fit_transform(X))\n",
        "Y = pd.DataFrame(Y)\n",
        "\n",
        "# Divide data into training, validation and test subsets\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, train_size=0.8, test_size=0.2, shuffle=True)\n",
        "X_valid, X_test, Y_valid, Y_test = train_test_split(X_valid, Y_valid, train_size=0.8, test_size=0.2,\n",
        "                                                    shuffle=True)\n",
        "train_set = (X_train, Y_train)\n",
        "print(f'X_train shape = {X_train.shape}')\n",
        "print(f'Y_trains shape = {Y_train.shape}')\n",
        "valid_set = (X_valid, Y_valid)\n",
        "print(f'X_valid shape = {X_valid.shape}')\n",
        "print(f'Y_valid shape = {Y_valid.shape}')\n",
        "test_set = (X_test, Y_test)\n",
        "print(f'X_test shape = {X_test.shape}')\n",
        "print(f'Y_test shape = {Y_test.shape}')\n",
        "\n",
        "\n",
        "def shared_dataset(data_xy, borrow=True):\n",
        "    \"\"\" Function that loads the dataset into shared variables\n",
        "\n",
        "    The reason we store our dataset in shared variables is to allow\n",
        "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
        "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
        "    is needed (the default behaviour if the data is not in a shared\n",
        "    variable) would lead to a large decrease in performance.\n",
        "    \"\"\"\n",
        "    data_x, data_y = data_xy\n",
        "    shared_x = theano.shared(numpy.asarray(data_x,\n",
        "                                           dtype=theano.config.floatX),\n",
        "                             borrow=borrow)\n",
        "    shared_y = theano.shared(numpy.asarray(data_y,\n",
        "                                           dtype=theano.config.floatX),\n",
        "                             borrow=borrow).flatten()\n",
        "    # When storing data on the GPU it has to be stored as floats\n",
        "    # therefore we will store the labels as ``floatX`` as well\n",
        "    # (``shared_y`` does exactly that). But during our computations\n",
        "    # we need them as ints (we use labels as index, and if they are\n",
        "    # floats it doesn't make sense) therefore instead of returning\n",
        "    # ``shared_y`` we will have to cast it to int. This little hack\n",
        "    # lets ous get around this issue\n",
        "    return shared_x, T.cast(shared_y, 'int32')\n",
        "\n",
        "\n",
        "te_set_x, te_set_y = shared_dataset(test_set)\n",
        "val_set_x, val_set_y = shared_dataset(valid_set)\n",
        "tr_set_x, tr_set_y = shared_dataset(train_set)\n",
        "dataset = [(tr_set_x, tr_set_y), (val_set_x, val_set_y), (te_set_x, te_set_y)]\n",
        "\n",
        "# MLP ==================================================================================================================\n",
        "#if __name__ == '__main__':\n",
        "#    test_mlp(dataset)\n",
        "\n",
        "# CNN ==================================================================================================================\n",
        "if __name__ == '__main__':\n",
        "    evaluate_lenet5(dataset)\n",
        "\n",
        "\n",
        "def experiment(state, channel):\n",
        "    evaluate_lenet5(state.learning_rate, dataset=state.dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape = (519, 47)\n",
            "Y_trains shape = (519, 1)\n",
            "X_valid shape = (104, 47)\n",
            "Y_valid shape = (104, 1)\n",
            "X_test shape = (26, 47)\n",
            "Y_test shape = (26, 1)\n",
            "... building the model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... training\n",
            "training @ iter =  0\n",
            "training @ iter =  100\n",
            "training @ iter =  200\n",
            "training @ iter =  300\n",
            "training @ iter =  400\n",
            "training @ iter =  500\n",
            "epoch 1, minibatch 519/519, validation error 10.576923 %\n",
            "     epoch 1, minibatch 519/519, test error of best model 11.538462 %\n",
            "training @ iter =  600\n",
            "training @ iter =  700\n",
            "training @ iter =  800\n",
            "training @ iter =  900\n",
            "training @ iter =  1000\n",
            "epoch 2, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  1100\n",
            "training @ iter =  1200\n",
            "training @ iter =  1300\n",
            "training @ iter =  1400\n",
            "training @ iter =  1500\n",
            "epoch 3, minibatch 519/519, validation error 11.538462 %\n",
            "training @ iter =  1600\n",
            "training @ iter =  1700\n",
            "training @ iter =  1800\n",
            "training @ iter =  1900\n",
            "training @ iter =  2000\n",
            "epoch 4, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  2100\n",
            "training @ iter =  2200\n",
            "training @ iter =  2300\n",
            "training @ iter =  2400\n",
            "training @ iter =  2500\n",
            "epoch 5, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  2600\n",
            "training @ iter =  2700\n",
            "training @ iter =  2800\n",
            "training @ iter =  2900\n",
            "training @ iter =  3000\n",
            "training @ iter =  3100\n",
            "epoch 6, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  3200\n",
            "training @ iter =  3300\n",
            "training @ iter =  3400\n",
            "training @ iter =  3500\n",
            "training @ iter =  3600\n",
            "epoch 7, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  3700\n",
            "training @ iter =  3800\n",
            "training @ iter =  3900\n",
            "training @ iter =  4000\n",
            "training @ iter =  4100\n",
            "epoch 8, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  4200\n",
            "training @ iter =  4300\n",
            "training @ iter =  4400\n",
            "training @ iter =  4500\n",
            "training @ iter =  4600\n",
            "epoch 9, minibatch 519/519, validation error 11.538462 %\n",
            "training @ iter =  4700\n",
            "training @ iter =  4800\n",
            "training @ iter =  4900\n",
            "training @ iter =  5000\n",
            "training @ iter =  5100\n",
            "epoch 10, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  5200\n",
            "training @ iter =  5300\n",
            "training @ iter =  5400\n",
            "training @ iter =  5500\n",
            "training @ iter =  5600\n",
            "training @ iter =  5700\n",
            "epoch 11, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  5800\n",
            "training @ iter =  5900\n",
            "training @ iter =  6000\n",
            "training @ iter =  6100\n",
            "training @ iter =  6200\n",
            "epoch 12, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  6300\n",
            "training @ iter =  6400\n",
            "training @ iter =  6500\n",
            "training @ iter =  6600\n",
            "training @ iter =  6700\n",
            "epoch 13, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  6800\n",
            "training @ iter =  6900\n",
            "training @ iter =  7000\n",
            "training @ iter =  7100\n",
            "training @ iter =  7200\n",
            "epoch 14, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  7300\n",
            "training @ iter =  7400\n",
            "training @ iter =  7500\n",
            "training @ iter =  7600\n",
            "training @ iter =  7700\n",
            "epoch 15, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  7800\n",
            "training @ iter =  7900\n",
            "training @ iter =  8000\n",
            "training @ iter =  8100\n",
            "training @ iter =  8200\n",
            "training @ iter =  8300\n",
            "epoch 16, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  8400\n",
            "training @ iter =  8500\n",
            "training @ iter =  8600\n",
            "training @ iter =  8700\n",
            "training @ iter =  8800\n",
            "epoch 17, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  8900\n",
            "training @ iter =  9000\n",
            "training @ iter =  9100\n",
            "training @ iter =  9200\n",
            "training @ iter =  9300\n",
            "epoch 18, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  9400\n",
            "training @ iter =  9500\n",
            "training @ iter =  9600\n",
            "training @ iter =  9700\n",
            "training @ iter =  9800\n",
            "epoch 19, minibatch 519/519, validation error 10.576923 %\n",
            "training @ iter =  9900\n",
            "training @ iter =  10000\n",
            "Optimization complete.\n",
            "Best validation score of 10.576923 % obtained at iteration 519, with test performance 11.538462 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-98a9283cb7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m# CNN ==================================================================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mevaluate_lenet5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6eb52f605c11>\u001b[0m in \u001b[0;36mevaluate_lenet5\u001b[0;34m(datasets, learning_rate, n_epochs, nkerns, batch_size)\u001b[0m\n\u001b[1;32m    299\u001b[0m           (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n\u001b[1;32m    300\u001b[0m     print(('The code for file ' +\n\u001b[0;32m--> 301\u001b[0;31m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m            ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}